version: '3.8'

# Production Docker Compose Configuration
# ==========================================
# WARNING: This configuration is for production use.
# Ensure all environment variables are properly set before deployment.
# DO NOT use default passwords or secrets in production!
# ==========================================

services:
  # PostgreSQL Database with Production Settings
  db:
    image: postgres:15-alpine
    container_name: keneyapp_db_prod
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${DB_USER:-keneyapp}
      POSTGRES_PASSWORD: ${DB_PASSWORD:?DB_PASSWORD must be set}
      POSTGRES_DB: ${DB_NAME:-keneyapp}
      # PostgreSQL performance tuning
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --lc-collate=C --lc-ctype=C"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    # Don't expose port in production - use internal network only
    # ports:
    #   - "5432:5432"
    networks:
      - keneyapp_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-keneyapp}"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Backend API with Production Settings
  backend:
    build:
      context: .
      dockerfile: Dockerfile.prod
      args:
        - BUILD_ENV=production
    container_name: keneyapp_backend_prod
    restart: unless-stopped
    environment:
      PYTHONPATH: /app
      DEBUG: "False"
      DATABASE_URL: postgresql://${DB_USER:-keneyapp}:${DB_PASSWORD:?DB_PASSWORD must be set}@db:5432/${DB_NAME:-keneyapp}
      SECRET_KEY: ${SECRET_KEY:?SECRET_KEY must be set}
      ALGORITHM: HS256
      ACCESS_TOKEN_EXPIRE_MINUTES: ${ACCESS_TOKEN_EXPIRE_MINUTES:-30}
      ALLOWED_ORIGINS: ${ALLOWED_ORIGINS:?ALLOWED_ORIGINS must be set}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_DB: 0
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      ENVIRONMENT: production
      GUNICORN_WORKERS: ${GUNICORN_WORKERS:-4}
      GUNICORN_TIMEOUT: ${GUNICORN_TIMEOUT:-120}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      # OAuth configuration (optional)
      GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID:-}
      GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET:-}
      MICROSOFT_CLIENT_ID: ${MICROSOFT_CLIENT_ID:-}
      MICROSOFT_CLIENT_SECRET: ${MICROSOFT_CLIENT_SECRET:-}
      APP_URL: ${APP_URL:?APP_URL must be set}
    # Don't expose port directly - use nginx as reverse proxy
    expose:
      - "8000"
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - keneyapp_network
    # Production: Don't mount source code
    # volumes:
    #   - ./app:/app/app
    command: >
      sh -c "alembic upgrade head &&
             gunicorn app.main:app
             --workers 4
             --worker-class uvicorn.workers.UvicornWorker
             --bind 0.0.0.0:8000
             --access-logfile -
             --error-logfile -
             --log-level info
             --timeout 120
             --keep-alive 5"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Frontend with Production Build
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
      args:
        - BUILD_ENV=production
        - REACT_APP_API_URL=${APP_URL}/api/v1
    container_name: keneyapp_frontend_prod
    restart: unless-stopped
    expose:
      - "80"
    depends_on:
      - backend
    networks:
      - keneyapp_network
    # Production: Serve built static files
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Redis Cache with Persistence
  redis:
    image: redis:7-alpine
    container_name: keneyapp_redis_prod
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 256mb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    volumes:
      - redis_data:/data
    networks:
      - keneyapp_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Celery Worker with Auto-scaling
  celery_worker:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: keneyapp_celery_worker_prod
    restart: unless-stopped
    environment:
      PYTHONPATH: /app
      DATABASE_URL: postgresql://${DB_USER:-keneyapp}:${DB_PASSWORD:?DB_PASSWORD must be set}@db:5432/${DB_NAME:-keneyapp}
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      ENVIRONMENT: production
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - keneyapp_network
    command: >
      celery -A app.core.celery_app worker
      --loglevel=info
      --concurrency=4
      --max-tasks-per-child=1000
      --time-limit=300
      --soft-time-limit=270
    healthcheck:
      test: ["CMD-SHELL", "celery -A app.core.celery_app inspect ping"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Celery Beat (Scheduler) with Persistence
  celery_beat:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: keneyapp_celery_beat_prod
    restart: unless-stopped
    environment:
      PYTHONPATH: /app
      DATABASE_URL: postgresql://${DB_USER:-keneyapp}:${DB_PASSWORD:?DB_PASSWORD must be set}@db:5432/${DB_NAME:-keneyapp}
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      ENVIRONMENT: production
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - keneyapp_network
    volumes:
      - celery_beat_data:/app/celerybeat
    command: >
      celery -A app.core.celery_app beat
      --loglevel=info
      --schedule=/app/celerybeat/schedule.db
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Flower (Celery Monitoring) - Protected with Authentication
  flower:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: keneyapp_flower_prod
    restart: unless-stopped
    environment:
      PYTHONPATH: /app
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: redis://redis:6379/0
      ENVIRONMENT: production
      FLOWER_BASIC_AUTH: ${FLOWER_USER:-admin}:${FLOWER_PASSWORD:?FLOWER_PASSWORD must be set}
    # Expose only to internal network or protect with nginx
    expose:
      - "5555"
    depends_on:
      - redis
      - celery_worker
    networks:
      - keneyapp_network
    command: >
      celery -A app.core.celery_app flower
      --port=5555
      --basic_auth=${FLOWER_USER:-admin}:${FLOWER_PASSWORD:?FLOWER_PASSWORD must be set}
      --url_prefix=flower
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: keneyapp_nginx_prod
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
    depends_on:
      - backend
      - frontend
    networks:
      - keneyapp_network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Prometheus Monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: keneyapp_prometheus_prod
    restart: unless-stopped
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    expose:
      - "9090"
    networks:
      - keneyapp_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Grafana Dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: keneyapp_grafana_prod
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:?GRAFANA_PASSWORD must be set}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource
      GF_SERVER_ROOT_URL: ${APP_URL}/grafana
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana-dashboard.json:/etc/grafana/provisioning/dashboards/keneyapp.json:ro
    expose:
      - "3000"
    depends_on:
      - prometheus
    networks:
      - keneyapp_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  celery_beat_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  nginx_logs:
    driver: local

networks:
  keneyapp_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
